---
title: "MLPerf Inference Benchmark"
collection: publications
permalink: /publication/2019-11-06-mlperf-inference-benchmark.md
excerpt: "Machine-learning (ML) hardware and software system demand is
burgeoning. Driven by ML applications, the number of different ML inference
systems has exploded. Over 100 organizations are building ML inference chips,
and the systems that incorporate existing models span at least three orders of
magnitude in power consumption and four orders of magnitude in performance;
they range from embedded devices to data-center solutions. Fueling the hardware
are a dozen or more software frameworks and libraries. The myriad combinations
of ML hardware and ML software make assessing ML-system performance in an
architecture-neutral, representative, and reproducible manner challenging.
There is a clear need for industry-wide standard ML benchmarking and evaluation
criteria. MLPerf Inference answers that call. Driven by more than 30
organizations as well as more than 200 ML engineers and practitioners, MLPerf
implements a set of rules and practices to ensure comparability across systems
with wildly differing architectures. In this paper, we present the method and
design principles of the initial MLPerf Inference release. The first call for
submissions garnered more than 600 inference-performance measurements from 14
organizations, representing over 30 systems that show a range of capabilities."
date: 2019-11-06
venue: arXiv
paperurl: "https://arxiv.org/abs/1911.02549"
---

[Download paper here](https://arxiv.org/abs/1911.02549)
